---
title: "Preparation of data from the 1000 Genomes Project"
format: html
---

Data for the table "Unity in Diversity: Genetic similarities and differences between regional populations in the 1000 Genomics Project" by Nathan Brouwer was prepared using  the code in this Quarto document.  This document was prepared in VSCode and is not setup to run using `reticulate` in RStudio.  The final table was prepared using `reticulate` and great_tables.

## Preliminaries

```{python}
import pandas as pd
import polars as pl
import great_tables as GT
import os
```


```{python}
os.listdir()
```


```{python}
my_vcf_gz_file = '15.27999021-28345461.ALL.chr15.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz'
bytes_to_mb = 1000000
os.path.getsize(my_vcf_gz_file)/bytes_to_mb
```


```{python}
run vcf_function_definitions.py
```


```{python}
# convert a .vcf.gz file to a a .vcf file
## all .gz files are compressed
## .vcf are a specially laid out text file
vcfgz_to_gz(vcf_gz_file = my_vcf_gz_file,
            vcf_name = "output.vcf")
```

```{python}
# vcf_file: your .vcf file
# csv_file: name of .csv file you want created
phased_snps = vcf_to_df(vcf_file = "output.vcf",
                        csv_file = "output_phased_snps.csv")
```

```{python}
phased_snps.shape
```


```{python}
# libraries
import time
import pandas as pd
import polars as pl

# target file
## file
csv_file = "output_phased_snps.csv"

## file size
bytes_to_mb = 1000000
print("file size = ",os.path.getsize(csv_file)/bytes_to_mb, "mb")

# load files
## load with pandas
start_time_pandas = time.time()
phased_snps_pd = pd.read_csv(csv_file)
pandas_time = round(time.time() - start_time_pandas,3)
print("--- %s seconds ---" % (pandas_time))

## load with polars
start_time_polars = time.time()
phased_snps_pl = pl.read_csv(csv_file)
polars_time = round(time.time() - start_time_polars,3)
print("--- %s seconds ---" % (polars_time))

print("Difference:", pandas_time-polars_time)

```


```{python}
phased_snps_pl.shape
```

```{python}
phased_snps_pl
```


```{python}
real_SNPs = ["A","T","C","G"]
onlyATCG_REF = phased_snps_pl["REF"].is_in(real_SNPs)

```

```{python}
phased_snps_biallelic = phased_snps_pl.filter(onlyATCG_REF)
```

```{python}
onlyATCG_ALT = phased_snps_biallelic["ALT"].is_in(real_SNPs)
```

```{python}
phased_snps_biallelic2 = phased_snps_biallelic.filter(onlyATCG_ALT)
```

```{python}
phased_snps_biallelic2.columns[0:10]
```
```{python}
phased_snps_biallelic2["REF"].value_counts()
phased_snps_biallelic2["ALT"].value_counts()
```


```{python}
phased_snps_biallelic2.columns[0:9]
```

```{python}
columns_to_drop = ['CHROM', 
                   #'POS', 
                   'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT']
phased_snps_biallelic2 = phased_snps_biallelic2.drop(columns_to_drop)
```



```{python}
phased_snps_biallelic2.columns[0:10]
```

```{python}
phased_snps_biallelic2.head()
```

```{python}
phased_snps_biallelic2.tail()
```

# Save file

```{python}
phased_snps_biallelic2.write_csv("phased_snps_biallelic2.csv")
```

```{python}
phased_snps_biallelic2.head()
```

# Featurize

```{python}
df_working = phased_snps_biallelic2
column_names = df_working.columns
phased = ["0|0","1|0","0|1","1|1"]
unphased = [0,1,1,2]
for column in column_names[1:]:
    new_col = df_working[column].replace(phased,unphased, return_dtype = pl.Int32)
    df_working.replace(column,new_col)

```


```{python}
featurized_snps = df_working
```

```{python}
featurized_snps.head()
```

```{python}
phased_snps_biallelic2.write_csv("featurized_snps.csv")
```

```{python}
phased_snps_biallelic2 = pl.read_csv("featurized_snps.csv")

```

```{python}
phased_snps_biallelic2 = phased_snps_biallelic2.with_columns(pl.col("POS").cast(pl.String))
```

```{python}
phased_snps_biallelic2.head()
```


# Transpose


```{python}
POS = phased_snps_biallelic2["POS"]
phased_snps_biallelic2 = phased_snps_biallelic2.drop("POS")

```

```{python}
phased_snps_biallelic2.head()
```

```{python}
phased_snps_biallelic2_T = phased_snps_biallelic2.transpose(include_header = True, 
header_name = "people")
```

```{python}
phased_snps_biallelic2_T.head()
```

```{python}
phased_snps_biallelic2_T.columns[1:] = POS[1:]
```

```{python}
phased_snps_biallelic2_T.head()
```

```{python}
phased_snps_biallelic2_T = phased_snps_biallelic2_T.drop_nulls()
```

```{python}
columns = phased_snps_biallelic2_T.columns[1:]
for column in columns:
    phased_snps_biallelic2_T = phased_snps_biallelic2_T.cast({column: pl.Int32})


```

```{python}
phased_snps_biallelic2_T.tail()
```

```{python}
phased_snps_biallelic2_T.head()
```

```{python}
noNANs = phased_snps_biallelic2_T.filter(
   pl.all_horizontal(pl.col(pl.Float32, pl.Float64).is_not_nan())
)
```

```{python}
noNANs.head()
```

```{python}
noNANs.shape
```

```{python}
csv_file = "noNANs.csv"
noNANs.write_csv(csv_file)
```

```{python}
noNANs = pl.read_csv("noNANs.csv")
```

```{python}
columns = noNANs.columns[1:]
```

```{python}
stand_devs = []
for i in range(1,len(columns)):
    stand_devs.append(noNANs.select(pl.nth(i)).std())
```



# Add people info

```{python}
people_file = "1000genomes_people_info.csv"
people = pl.read_csv(people_file)
```

```{python}
people = people.drop(["sex","lat","lng"])
```

```{python}
people.columns = ["pop","sup_pop","people"]
```
```{python}
merged_data = noNANs.join(people, on = "people")
```

```{python}
merged_data = merged_data.drop("people")
```

```{python}
merged_data.columns
```

```{python}
merged_data.write_csv("merged_data.csv")
```

```{python}
columns = merged_data.columns
columns.drop("pop")
```
```{python}
for column in columns:
    if column == "pop":
        continue
    if column == "sup_pop":
        continue
    merged_data01 = merged_data.with_columns(
        pl.when(pl.col(column) > 0)
        .then(1))
```

```{python}
merged_data01["column_10"].value_counts()
```

## Calcualte group means


```{python}
group_means = merged_data.group_by(["sup_pop","pop"], maintain_order=True).mean()
```

```{python}
group_means = group_means.sort(by = "sup_pop")
```

```{python}
group_means
```


```{python}
import numpy as np
from sklearn.metrics.pairwise import pairwise_distances

pop = group_means["pop"]
sup_pop = group_means["sup_pop"]
group_means2 = group_means.drop(["pop","sup_pop"])
pairwise_distr_pop = pairwise_distances(group_means2, metric='euclidean')
```

```{python}
pairwise_distr_pop
```

```{python}
dist_mat_super_pop = pl.DataFrame(data=pairwise_distr_pop)
```

```{python}
dist_mat_super_pop.columns = pop
```

```{python}
dist_mat_super_pop
```

```{python}

```
```{python}
dist_mat = dist_mat_super_pop.with_columns(pop = pop,sup_pop = sup_pop)


```

```{python}
columns = ['sup_pop',
 'pop','ACB',
 'ASW',
 'ESN',
 'GWD',
 'LWK',
 'MSL',
 'YRI',
 'CLM',
 'MXL',
 'PEL',
 'PUR',
 'CDX',
 'CHB',
 'CHS',
 'JPT',
 'KHV',
 'CEU',
 'FIN',
 'GBR',
 'IBS',
 'TSI',
 'BEB',
 'GIH',
 'ITU',
 'PJL',
 'STU']
 
```

```{python}
dist_mat2= dist_mat.select(columns)
```


```{python}
dist_mat2
```


```{python}
dist_mat2.write_csv("distance_matrix.csv")
```



